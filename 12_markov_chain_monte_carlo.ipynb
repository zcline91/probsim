{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Markov chain Monte Carlo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to implement the Metropolis-Hastings algorithm for Example 12.1.8, the Normal-Normal model. First, we choose our observed value of $Y$ and decide on values for the constants $\\sigma$, $\\mu$ and $\\tau$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y <- 3\n",
    "sigma <- 1\n",
    "mu <- 0\n",
    "tau <- 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to choose the standard deviation of the proposals for step $1$ of the algorithm, as explained in Example 12.1.8; for this problem, we let $d= 1$. We set the number of iterations to run, and we allocate a vector `theta` of length $10^4$ which we will fill with our simulated draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "d <- 1\n",
    "niter <- 10^4\n",
    "theta <- rep(0,niter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the main loop. We initialize $\\theta$ to the observed value $y$, then run the algorithm described in Example 12.1.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "theta[1] <- y\n",
    "for (i in 2:niter){\n",
    "    theta.p <- theta[i-1] + rnorm(1,0,d)\n",
    "    r <- dnorm(y,theta.p,sigma) * dnorm(theta.p,mu,tau) /\n",
    "        (dnorm(y,theta[i-1],sigma) * dnorm(theta[i-1],mu,tau))\n",
    "    flip <- rbinom(1,1,min(r,1))\n",
    "    theta[i] <- if(flip==1) theta.p else theta[i-1]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through each line inside the loop. The proposed value of $\\theta$ is `theta.p`, which equals the previous value of $\\theta$ plus a Normal random variable with mean $0$ and standard deviation `d` (recall that `rnorm` takes the standard deviation and not the variance as input). The ratio `r` is\n",
    "$$ \\frac{f_{\\theta|Y}(x'|y)}{f_{\\theta|Y}(x|y)} = \\frac{e^{- \\frac{1}{2 \\sigma^2}(y-x')^2} e^{-\\frac{1}{2\\tau^2}(x' - m)^2}}{e^{- \\frac{1}{2 \\sigma^2}(y-x)^2} e^{-\\frac{1}{2\\tau^2}(x - m)^2}},$$\n",
    "where `theta.p` is playing the role of $x'$ and `theta[i-1]` is playing the role of $x$. The coin flip to determine whether to accept or reject the proposal is `flip`, which is a coin flip with probability `min(r,1)` of Heads (encoding Heads as $1$ and Tails as $0$). Finally, we set `theta[i]` equal to the proposed value if the coin flip lands Heads, and keep it at the previous value otherwise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `theta` now contains all of our simulation draws. We typically discard some of the initial draws to give the chain some time to approach the stationary distribution. The following line of code discards the first half of the draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "theta <- theta[-(1:(niter/2))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the remaining draws look like, we can create a histogram using `hist(theta)`. We can also compute summary statistics such as `mean(theta)` and `var(theta)`, which give us the sample mean and sample variance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement Gibbs sampling for Example 12.2.6, the chicken-egg story with unknown hatching probability and invisible unhatched eggs. The first step is to decide on our observed value of $X$, as well as the constants $\\lambda,a,b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- 7\n",
    "lambda <- 10\n",
    "a <- 1\n",
    "b <- 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we decide how many iterations to run, and we allocate space for our results, creating two vectors `p` and `N` of length $10^4$ which we will fill with our simulated draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "niter <- 10^4\n",
    "p <- rep(0,niter)\n",
    "N <- rep(0,niter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to run the Gibbs sampler. We initialize $p$ and $N$ to the values $0.5$ and $2x$, respectively, and then we run the algorithm as explained in Example 12.2.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p[1] <- 0.5\n",
    "N[1] <- 2*x\n",
    "for (i in 2:niter){\n",
    "    p[i] <- rbeta(1,x+a,N[i-1]-x+b)\n",
    "    N[i] <- x + rpois(1,lambda*(1-p[i-1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we discard the initial draws:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "p <- p[-(1:(niter/2))]\n",
    "N <- N[-(1:(niter/2))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the remaining draws look like, we can make histograms using `hist(p)` and `hist(N)`, which is how we created Figure 12.5. We can also compute summary statistics such as `mean(p)` or `median(p)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
